# -*- coding: utf-8 -*-
"""ConvAI_Group_111_RAG_vs_FT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WuNXl__DNwjeSNyRq6kdAINtrl6OCKuW

# Assignment 2 â€“ Comparative Financial QA System: RAG vs Fine-Tuning

## Objective
Develop and compare two systems for answering questions based on company financial statements (last two years):

- **Retrieval-Augmented Generation (RAG) Chatbot**: Combines document retrieval and generative response.  
- **Fine-Tuned Language Model (FT) Chatbot**: Directly fine-tunes a small open-source language model on financial Q&A.  

Use the same financial data for both methods and perform a detailed comparison on **accuracy, speed, and robustness**.

---

## Step-by-Step Tasks

### 1. Data Collection & Preprocessing
1. Obtain financial statements for the last two years (publicly available or from a group memberâ€™s company).  
2. Convert documents (PDF, Excel, HTML) to plain text using OCR or appropriate parsers.  
3. Clean text by removing noise like headers, footers, and page numbers.  
4. Segment reports into logical sections (e.g., income statement, balance sheet).  
5. Construct at least **50 Q/A pairs** reflecting the financial data.  

**Example:**  
- Q: *What was the companyâ€™s revenue in 2023?*  
- A: *The companyâ€™s revenue in 2023 was $4.13 billion.*  

---

### 2. Retrieval-Augmented Generation (RAG) System Implementation

#### 2.1 Data Processing
- Split the cleaned text into chunks (â‰¥2 sizes, e.g., 100 and 400 tokens).  
- Assign unique IDs and metadata to chunks.  

#### 2.2 Embedding & Indexing
- Embed chunks using a small open-source sentence embedding model (e.g., `all-MiniLM-L6-v2`, `E5-small-v2`).  
- Build:
  - Dense vector store (e.g., FAISS, ChromaDB).  
  - Sparse index (BM25 or TF-IDF).  

#### 2.3 Hybrid Retrieval Pipeline
For each user query:
1. Preprocess (clean, lowercase, stopword removal).  
2. Generate query embedding.  
3. Retrieve top-N chunks from:
   - Dense retrieval (vector similarity).  
   - Sparse retrieval (BM25).  
4. Combine results (union or weighted fusion).  

#### 2.4 Advanced RAG Technique (Select One)

| Remainder (Group# mod 5) | Advanced Technique              | Description |
|---------------------------|----------------------------------|-------------|
| 1 | Multi-Stage Retrieval | Stage 1: Broad retrieval; Stage 2: Re-rank with cross-encoder. |
| 2 | Chunk Merging & Adaptive Retrieval | Merge adjacent chunks/adapt size based on query. |
| 3 | Re-Ranking with Cross-Encoders | Re-rank retrieved chunks by query relevance. |
| 4 | Hybrid Search | Combine BM25 + dense retrieval for balanced recall/precision. |
| 0 | Memory-Augmented Retrieval | Add persistent memory of frequent/important Q&A. |

#### 2.5 Response Generation
- Use a small open-source generative model (e.g., `DistilGPT2`, `GPT-2 Small`, `Llama-2 7B`).  
- Input = retrieved passages + query.  
- Limit tokens to model context window.  

#### 2.6 Guardrail Implementation
- **Input-side**: Validate queries (filter irrelevant/harmful).  
- **Output-side**: Filter hallucinated/non-factual outputs.  

#### 2.7 Interface Development
Build UI (Streamlit, Gradio, CLI, GUI) with features:
- Accept query  
- Display: answer, confidence, method, response time  
- Switch between RAG and FT modes  

---

### 3. Fine-Tuned Model System Implementation

#### 3.1 Q/A Dataset Preparation
- Use the same ~50 Q/A pairs.  
- Convert into fine-tuning dataset format.  

#### 3.2 Model Selection
- Choose a small open-source model (e.g., DistilBERT, MiniLM, GPT-2 Small/Medium, Llama-2 7B, Falcon 7B, Mistral 7B).  
- **No closed/proprietary APIs.**  

#### 3.3 Baseline Benchmarking
- Evaluate pre-trained model on â‰¥10 test questions.  
- Record accuracy, confidence, and inference speed.  

#### 3.4 Fine-Tuning
- Fine-tune model on Q/A dataset.  
- Log hyperparameters: learning rate, batch size, epochs, compute setup.  
- Use assigned efficient fine-tuning method.  

#### 3.5 Advanced Fine-Tuning Technique (Select One)

| Remainder (Group# mod 5) | Technique | Description |
|---------------------------|-----------|-------------|
| 1 | Supervised Instruction Fine-Tuning | Train on instruction-style Q/A. |
| 2 | Adapter-Based Tuning | Insert adapter modules for efficiency. |
| 3 | Mixture-of-Experts Fine-Tuning | Multi-expert architectures for tuning/inference. |
| 4 | Retrieval-Augmented Fine-Tuning | Combine retrieval with fine-tuning. |
| 0 | Continual Learning / Domain Adaptation | Incrementally adapt to new financial data. |

#### 3.6 Guardrail Implementation
- Input/output guardrails (similar to RAG).  

#### 3.7 Interface Development
- Integrate FT model into same UI.  
- Display: answer, confidence, method, inference time.  
- Allow switching between methods.  

---

### 4. Testing, Evaluation & Comparison

#### 4.1 Test Questions (Mandatory)
- **Relevant, high-confidence**: Clear fact in data.  
- **Relevant, low-confidence**: Ambiguous/sparse info.  
- **Irrelevant**: e.g., *What is the capital of France?*  

#### 4.2 Extended Evaluation
- Evaluate â‰¥10 financial questions.  
- Record for each system:
  - Real answer  
  - Model answer  
  - Confidence  
  - Response time  
  - Correctness (Y/N)  

#### 4.3 Results Table (Example)

| Question | Method | Answer | Confidence | Time (s) | Correct (Y/N) |
|----------|--------|--------|------------|----------|---------------|
| Revenue in 2023? | RAG | $4.02B | 0.92 | 0.50 | Y |
| Revenue in 2023? | Fine-Tune | $4.13B | 0.93 | 0.41 | Y |
| Unique products? | RAG | 13,000 units | 0.81 | 0.79 | Y |
| Unique products? | Fine-Tune | 13,240 units | 0.89 | 0.65 | Y |
| Capital of France? | RAG | Data not in scope | 0.35 | 0.46 | Y |
| Capital of France? | Fine-Tune | Not applicable | 0.85 | 0.38 | Y |

#### 4.4 Analysis
- Compare average inference speed and accuracy.  
- Discuss:
  - Strengths of RAG (adaptability, factual grounding).  
  - Strengths of FT (fluency, efficiency).  
  - Robustness to irrelevant queries.  
  - Practical trade-offs.  

---

### 5. Submission Requirements
Submit a **ZIP file per group**: `Group_<Number>_RAG_vs_FT.zip`

**Contents:**
1. Python Notebook (.ipynb/.py) with:
   - Data processing  
   - RAG + FT implementations  
   - Advanced technique sections  
   - Testing & comparison tables  
2. PDF Report with:
   - 3 screenshots (queries, answers, confidence, time, method)  
   - Summary comparison table  
   - Hosted demo app link  
3. Use **open-source only** (no proprietary APIs).  
4. Comment & document all code clearly.  

---

## Notes & Recommendations
- Use free/institutional GPU resources (Colab, Kaggle, campus clusters).  
- Quantitative comparison + documentation are critical.  
- Implement **guardrails** (mandatory).  
- UI should be user-friendly and show which method is used.  

---

ðŸ’¡ **Goal**: Hands-on experience in building & comparing RAG vs Fine-Tuning for specialized **financial Q&A** using open-source technologies.  
Good luck!

===============================================================================
RAG-Based QA System with Guardrails (Colab Notebook)
===============================================================================

# Install and Import Libraries
"""

import os
import re
import time
import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
from dataclasses import dataclass, asdict
import nltk
import warnings
warnings.filterwarnings("ignore")
nltk.download('punkt_tab')

# Core ML libraries
import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM
)
from sentence_transformers import SentenceTransformer, CrossEncoder
import faiss
from rank_bm25 import BM25Okapi

# UI libraries
import gradio as gr

# Text processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Optional .docx support
try:
    from docx import Document as DocxDocument
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False

try:
    import pdfplumber
    HAS_PDFPLUMBER = True
except ImportError:
    HAS_PDFPLUMBER = False

# Download required NLTK data (idempotent)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)


def _read_txt(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return path.read_text(encoding="latin-1", errors="ignore")



def _read_docx(path: Path) -> Tuple[str, List[Dict]]:
    """
    Extract text and structured tables from a .docx file.
    Returns:
        full_text (str): flattened text for embeddings
        tables_json (List[Dict]): structured representation of tables
    """
    if not HAS_DOCX:
        raise RuntimeError("python-docx is not installed. Please install with: pip install python-docx")

    doc = DocxDocument(str(path))

    # --- Extract paragraphs ---
    paras = [p.text for p in doc.paragraphs if p.text and p.text.strip()]
    full_text = "\n".join(paras)

    # --- Extract tables ---
    tables_json = []
    for table_idx, table in enumerate(doc.tables):
        table_data = []
        for row in table.rows:
            row_data = [cell.text.strip() for cell in row.cells]
            table_data.append(row_data)

        # Convert rows to dict using first row as headers
        if table_data:
            headers = table_data[0]
            for row_num, row in enumerate(table_data[1:], start=1):
                row_json = {headers[i]: row[i] for i in range(min(len(headers), len(row)))}
                row_json.update({'table_index': table_idx, 'row_index': row_num, 'type': 'table_row', 'source': row})
                tables_json.append(row_json)

    return full_text, tables_json





def _read_pdf(path: Path) -> Tuple[str, List[Dict]]:
    """
    Extract text and tables from PDF.
    Returns:
        full_text (str)
        tables_json (List[Dict])
    """
    if not HAS_PDFPLUMBER:
        raise RuntimeError("pdfplumber is not installed. Please install with: pip install pdfplumber")

    text = []
    tables_json = []

    with pdfplumber.open(str(path)) as pdf:
        for page in pdf.pages:
            # Extract text
            content = page.extract_text()
            if content:
                text.append(content.strip())

            # Extract tables
            tables = page.extract_tables()
            for tbl in tables:
                if not tbl:
                    continue
                headers = tbl[0]
                for row in tbl[1:]:
                    row_json = {headers[i]: row[i] for i in range(min(len(headers), len(row)))}
                    tables_json.append(row_json)

    return "\n".join(text), tables_json

"""
def load_files_to_strings(paths: List[Path]) -> List[str]:
    docs = []
    for p in paths:
        suffix = p.suffix.lower()
        try:
            if suffix == ".txt":
                docs.append(_read_txt(p))
            elif suffix == ".docx":
                full_text, tables_json = _read_docx(p)
                docs.append(full_text)
                # Optionally, include table rows as separate entries for retrieval
                for row in tables_json:
                    # Flatten table row to string for indexing
                    row_text = ' | '.join(f'{k}: {v}' for k, v in row.items() if k not in ['table_index', 'row_index', 'type', 'source'])
                    docs.append(row_text)
            elif suffix == ".pdf":
                docs.append(_read_pdf(p))
            else:
                # silently skip unsupported file types
                pass
        except Exception as e:
            print(f"Failed to read {p}: {e}")
    return docs
"""
def load_files_to_strings(paths: List[Path]) -> List[Dict]:
    docs = []
    for p in paths:
        suffix = p.suffix.lower()
        try:
            if suffix == ".txt":
                text = _read_txt(p)
                docs.append({"text": text, "metadata": {"source": str(p)}, "score": 0.0})
            elif suffix == ".docx":
                full_text, tables_json = _read_docx(p)
                docs.append({"text": full_text, "metadata": {"source": str(p)}, "score": 0.0})
                # Include table rows as separate entries
                for row in tables_json:
                    row_text = ' | '.join(f'{k}: {v}' for k, v in row.items() if k not in ['table_index', 'row_index', 'type', 'source'])
                    docs.append({"text": row_text, "metadata": {"source": str(p)}, "score": 0.0})
            elif suffix == ".pdf":
                text = _read_pdf(p)
                docs.append({"text": text, "metadata": {"source": str(p)}, "score": 0.0})
            else:
                pass
        except Exception as e:
            print(f"Failed to read {p}: {e}")
    return docs

"""#  Data Processing"""

def _chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """Simple word-wise chunking with overlap."""
    words = text.split()
    if chunk_size <= 0:
        return [text]
    chunks = []
    start = 0
    step = max(1, chunk_size - chunk_overlap)
    while start < len(words):
        end = min(len(words), start + chunk_size)
        chunks.append(' '.join(words[start:end]))
        if end == len(words):
            break
        start += step
    return chunks


def _normalize_space(s: str) -> str:
    return re.sub(r'\s+', ' ', s).strip()


def _remove_repeated_phrases(text: str) -> str:
    """
    Remove common repeated artefacts and collapse long repetitions.
    Example: 'change in accounting estimate' repeated many times.
    """
    patterns = [
        r'(change in accounting estimate[\.\s,;:]*){2,}',
        r'(forward-looking statements[\.\s,;:]*){2,}',
        r'(safe harbor statement[\.\s,;:]*){2,}',
    ]
    out = text
    for p in patterns:
        out = re.sub(p, lambda m: _normalize_space(m.group(0).split()[0:4] and " ".join(m.group(0).split()[0:4])), out, flags=re.IGNORECASE)
    # Collapse repeated lines
    lines = [ln.strip() for ln in out.splitlines() if ln.strip()]
    deduped = []
    seen = set()
    for ln in lines:
        norm = _normalize_space(ln.lower())
        if norm in seen:
            continue
        seen.add(norm)
        deduped.append(ln)
    return _normalize_space("\n".join(deduped))


def clean_documents(docs: List[str]) -> List[str]:
    cleaned = []
    seen = set()
    for doc in docs:
        text = _normalize_space(doc)
        text = _remove_repeated_phrases(text)
        if not text:
            continue
        if text in seen:
            continue
        seen.add(text)
        cleaned.append(text)
    return cleaned

"""# RAG CONSTRUCTION

RAG configuration File all the detals of constants used
"""

@dataclass
class RAGConfig:
    """Configuration for the complete RAG pipeline"""
    # Embedding settings
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    cross_encoder_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"

    # Generation settings
    generator_model: str = "distilgpt2"  # or "gpt2" or "microsoft/DialoGPT-small"
    max_context_length: int = 512
    max_generation_length: int = 150

    # Retrieval settings
    initial_retrieval_k: int = 20  # Stage 1: broad retrieval
    final_retrieval_k: int = 5     # Stage 2: after re-ranking
    hybrid_alpha: float = 0.7      # Dense vs sparse weight

    # Processing settings
    chunk_size: int = 300
    chunk_overlap: int = 50
    batch_size: int = 16

    # Guardrail settings
    enable_input_guardrails: bool = True
    enable_output_guardrails: bool = True
    max_query_length: int = 500
    min_answer_length: int = 10

    # Interface settings
    interface_type: str = "gradio"  # "gradio" or "streamlit"

"""# Query Processor"""

class QueryPreprocessor:
    """Handles query preprocessing with multiple cleaning strategies"""

    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        # Add custom domain-specific stop words
        self.stop_words.update(['would', 'could', 'should', 'might', 'may'])

    def clean_query(self, query: str) -> str:
        """Basic cleaning: remove special chars, normalize whitespace"""
        query = re.sub(r'[^\w\s\?\.\!,]', ' ', query)
        query = re.sub(r'\s+', ' ', query).strip()
        return query

    def remove_stopwords(self, query: str) -> str:
        """Remove stopwords while preserving question structure"""
        tokens = word_tokenize(query.lower())
        question_words = {'what', 'when', 'where', 'who', 'why', 'how', 'which'}

        filtered_tokens = []
        for token in tokens:
            if token not in self.stop_words or token in question_words:
                if token not in string.punctuation:
                    filtered_tokens.append(token)

        return ' '.join(filtered_tokens)

    def expand_query(self, query: str) -> List[str]:
        """Generate query variations for better retrieval"""
        variations = [query]

        cleaned = self.clean_query(query)
        if cleaned != query:
            variations.append(cleaned)

        no_stopwords = self.remove_stopwords(query)
        if no_stopwords and no_stopwords != query.lower():
            variations.append(no_stopwords)

        return variations

    def preprocess(self, query: str) -> Dict[str, Union[str, List[str]]]:
        """Complete preprocessing pipeline"""
        return {
            'original': query,
            'cleaned': self.clean_query(query),
            'no_stopwords': self.remove_stopwords(query),
            'variations': self.expand_query(query)
        }

"""   # Multi Stage Retrieval
    
    Advanced RAG Technique: Multi-Stage Retrieval
    Stage 1: Broad retrieval using hybrid search
    Stage 2: Precise re-ranking using cross-encoder
"""

class MultiStageRetriever:
    """
    Advanced RAG Technique: Multi-Stage Retrieval
    Stage 1: Broad retrieval using hybrid search
    Stage 2: Precise re-ranking using cross-encoder
    """

    def __init__(self, config: RAGConfig):
        self.config = config

        print("Loading retrieval models...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.embedding_model = SentenceTransformer(config.embedding_model, device=device)
        self.cross_encoder = CrossEncoder(config.cross_encoder_model, device=device)

        self.dense_index = None
        self.sparse_index = None
        self.documents: List[str] = []
        self.metadata: List[Dict] = []

        print("Multi-stage retriever initialized")

    def build_indices(self, documents: List[str], metadata: Optional[List[Dict]] = None):
        """Build both dense and sparse indices"""
        print(f"Building indices for {len(documents)} documents...")

        # Normalize documents -> always text
        flat_docs = []
        enriched_metadata = []

        for i, doc in enumerate(documents):
          if isinstance(doc, dict):
             if 'text' in doc:
                flat_docs.append(doc['text'])
                meta=doc.get('metadata', {'id': i, 'type': 'paragraph', 'preview': doc['text'][:100]})
                #enriched_metadata.append(doc.get('metadata', {'id': i, 'type': 'paragraph', 'preview': doc['text'][:100]}))
             else:
                # Treat as table row dict, convert to readable text
                doc_text = ' | '.join(f"{k}: {v}" for k, v in doc.items() if v)
                flat_docs.append(doc_text)
                meta={'id': i, 'type': 'table_row', 'source': doc}
                #enriched_metadata.append({'id': i, 'type': 'table_row', 'source': doc})
          else:
            # plain string
            flat_docs.append(doc)
            meta={'id': i, 'type': 'paragraph', 'preview': str(doc)[:100]}
            #enriched_metadata.append({'id': i, 'type': 'paragraph', 'preview': doc[:100]})

          meta['score'] = 0.0
          enriched_metadata.append(meta)

        """
        for i, doc in enumerate(documents):
           if isinstance(doc, dict):
              # Convert row-dict into readable text
               doc_text = " | ".join(f"{k}: {v}" for k, v in doc.items() if v)
               flat_docs.append(doc_text)
               enriched_metadata.append({"id": i, "type": "table_row", "source": doc})
           else:
               flat_docs.append(doc)
               enriched_metadata.append({"id": i, "type": "paragraph", "preview": doc[:100]})
        """
        self.documents = flat_docs
        self.metadata = metadata or enriched_metadata
        # Initialize default score for each document
        self.scores = [0.0 for _ in flat_docs]

        #self.documents = documents
        #self.metadata = metadata or [{"id": i, "preview": doc[:100]} for i, doc in enumerate(documents)]

        # Dense index (FAISS)
        print("Creating embeddings...")
        embeddings = self.embedding_model.encode(
            documents,
            batch_size=self.config.batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True,  # cosine via inner product
        ).astype(np.float32)

        dimension = embeddings.shape[1]
        self.dense_index = faiss.IndexFlatIP(dimension)
        self.dense_index.add(embeddings)

        # Sparse index (BM25)
        print("Building BM25 index...")
        tokenized_docs = [doc.lower().split() for doc in documents]
        self.sparse_index = BM25Okapi(tokenized_docs)

        print(f"Indices built: {len(documents)} documents indexed")

    def _revenue_boost(self, query: str, docs: List[Dict]) -> List[Dict]:
        """Boost docs mentioning revenue + numbers when query relates to revenue/financials"""
        if not re.search(r"\brevenue|income|sales|earnings\b", query.lower()):
            return docs  # no boost needed

        boosted = []
        for d in docs:
            score = d.get('score', d.get('hybrid_score', d.get('cross_encoder_score', 0.0)))
            text = d["text"]

            # Look for revenue + number in same chunk
            if re.search(r"\brevenue\b", text, re.IGNORECASE) and re.search(r"[\$â‚¬â‚¹]?\s?\d[\d,\.]+\s?(million|billion|trillion)?", text, re.IGNORECASE):
                score += 2.0  # strong boost


            # --- Case 2: Table-style entries (e.g. "Total revenue .... 211,000,000") ---
            elif re.search(r"(total\s+)?revenue", text, re.IGNORECASE) and re.search(r"\d{3,}(?:,\d{3})*(?:\.\d+)?",text, re.IGNORECASE):
                score += 2.0


            # Slight bump for any financial keyword
            elif re.search(r"\b(income|profit|sales|total)\b", text, re.IGNORECASE):
                score += 0.5

            d['score'] = score  # ensure score is always set
            boosted.append({**d, "score": score})

        # Re-sort by boosted score
        boosted = sorted(boosted, key=lambda x: x["score"], reverse=True)
        return boosted

    def stage1_broad_retrieval(self, query: str, k: Optional[int] = None) -> List[Dict]:
        k = k or self.config.initial_retrieval_k

        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True, normalize_embeddings=True).astype(np.float32)
        dense_scores, dense_indices = self.dense_index.search(query_embedding, min(k, len(self.documents)))
        dense_results = []
        for score, idx in zip(dense_scores[0], dense_indices[0]):
            if 0 <= idx < len(self.documents):
                dense_results.append({
                    'index': int(idx),
                    'dense_score': float(score),
                    'sparse_score': 0.0,
                    'text': self.documents[idx],
                    'metadata': self.metadata[idx]
                })

        # Sparse retrieval (BM25)
        query_tokens = query.lower().split()
        sparse_scores = self.sparse_index.get_scores(query_tokens)
        if sparse_scores is None or len(sparse_scores) == 0:
            sparse_top_indices = []
        else:
            sparse_top_indices = np.argsort(sparse_scores)[::-1][:k]

        alpha = self.config.hybrid_alpha
        combined_results: Dict[int, Dict] = {}

        for result in dense_results:
            combined_results[result['index']] = result

        for idx in sparse_top_indices:
            sparse_score = float(sparse_scores[idx])
            if idx in combined_results:
                combined_results[idx]['sparse_score'] = sparse_score
            else:
                combined_results[idx] = {
                    'index': int(idx),
                    'dense_score': 0.0,
                    'sparse_score': sparse_score,
                    'text': self.documents[idx],
                    'metadata': self.metadata[idx]
                }

        # Normalize sparse scores
        max_sparse = float(max(sparse_scores)) if isinstance(sparse_scores, (list, np.ndarray)) and len(sparse_scores) > 0 else 1.0
        max_sparse = max(max_sparse, 1e-8)

        for r in combined_results.values():
            dense_norm = r['dense_score']  # already cosine-ish
            sparse_norm = r['sparse_score'] / max_sparse
            r['hybrid_score'] = alpha * dense_norm + (1 - alpha) * sparse_norm
            r['stage'] = 'broad_retrieval'

        sorted_results = sorted(combined_results.values(), key=lambda x: x['hybrid_score'], reverse=True)
        return sorted_results[:k]

    def stage2_precise_reranking(self, query: str, candidates: List[Dict], k: Optional[int] = None) -> List[Dict]:
        k = k or self.config.final_retrieval_k
        if not candidates:
            return []

        print(f"Re-ranking {len(candidates)} candidates with cross-encoder...")
        query_doc_pairs = []
        for candidate in candidates:
            doc_text = candidate['text'][:500]
            query_doc_pairs.append([query, doc_text])

        try:
            cross_encoder_scores = self.cross_encoder.predict(query_doc_pairs)
            for i, candidate in enumerate(candidates):
                candidate['cross_encoder_score'] = float(cross_encoder_scores[i])
                candidate['stage'] = 'precise_reranking'
            reranked = sorted(candidates, key=lambda x: x['cross_encoder_score'], reverse=True)
            return reranked[:k]
        except Exception as e:
            print(f"Cross-encoder re-ranking failed: {e}")
            return candidates[:k]

    def multi_stage_retrieve(self, query: str) -> List[Dict]:

        # Stage 1: broad retrieval
        start_time = time.time()
        stage1_results = self.stage1_broad_retrieval(query, self.config.initial_retrieval_k)
        stage1_time = time.time()

        # Stage 2: precise reranking
        final_results = self.stage2_precise_reranking(query, stage1_results, self.config.final_retrieval_k)
        stage2_time = time.time()

        # Stage 3: financial boosting
        final_results = self._revenue_boost(query, final_results)

        enriched_results = []
        for i, result in enumerate(final_results):
            meta = result.get("metadata", {})
            enriched = {
            "final_rank": i + 1,
            "stage1_time": stage1_time - start_time,
            "stage2_time": stage2_time - stage1_time,
            "total_time": stage2_time - start_time,
            "score": result.get("cross_encoder_score", result.get("hybrid_score", 0.0)),
            "text": result["text"],
            "metadata": meta
             }

            #result['final_rank'] = i + 1
            #result['stage1_time'] = stage1_time - start_time
            #result['stage2_time'] = stage2_time - stage1_time
            #result['total_time'] = stage2_time - start_time
                    # If the document comes from a table row, include JSON row
            if meta.get("type") == "table_row":
                enriched["table_row"] = meta.get("source")

            enriched_results.append(enriched)

        #print(f"Multi-stage retrieval completed: {len(final_results)} results")
        #return final_results
        print(f"Multi-stage retrieval completed: {len(enriched_results)} results")
        return enriched_results

"""   # ResponseGenerator  
  
  Handles response generation with context management
"""

class ResponseGenerator:
    """Handles response generation with context management"""
    def __init__(self, config: RAGConfig):
        self.config = config

        print(f"Loading generation model: {config.generator_model}")
        self.tokenizer = AutoTokenizer.from_pretrained(config.generator_model)
        self.model = AutoModelForCausalLM.from_pretrained(config.generator_model)

        # Proper padding configuration for GPT2-like models
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        if getattr(self.model.config, "pad_token_id", None) is None:
            self.model.config.pad_token_id = self.tokenizer.pad_token_id

        print("Response generator initialized")

    def prepare_context(self, query: str, retrieved_docs: List[Dict]) -> str:
        context_parts = []
        current_tokens = 0
        max_context_tokens = max(64, self.config.max_context_length - 100)  # buffer for query + prompt

        for doc in retrieved_docs:
            doc_text = doc['text']
            doc_tokens = len(self.tokenizer.encode(doc_text, add_special_tokens=False))
            if current_tokens + doc_tokens <= max_context_tokens:
                context_parts.append(f"Document {len(context_parts)+1}: {doc_text}")
                current_tokens += doc_tokens
            else:
                remaining_tokens = max_context_tokens - current_tokens
                if remaining_tokens > 50:
                    truncated_tokens = self.tokenizer.encode(doc_text, add_special_tokens=False)[:remaining_tokens]
                    truncated_text = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
                    context_parts.append(f"Document {len(context_parts)+1}: {truncated_text}...")
                break

        return "\n\n".join(context_parts)

    def generate_response(self, query: str, retrieved_docs: List[Dict]) -> Dict[str, Union[str, int]]:
        context = self.prepare_context(query, retrieved_docs)
        prompt = (
            "Based on the following context, answer the question accurately and concisely.\n\n"
            f"Context:\n{context}\n\n"
            f"Question: {query}\n\n"
            "Answer:"
        )

        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.config.max_context_length,
                truncation=True,
                padding=True
            )
            attention_mask = inputs.get("attention_mask", None)

            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=attention_mask,
                    max_new_tokens=self.config.max_generation_length,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    no_repeat_ngram_size=3,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            if "Answer:" in generated_text:
                answer = generated_text.split("Answer:")[-1].strip()
            else:
                answer = generated_text[len(prompt):].strip()

            return {
                'answer': answer,
                'prompt_tokens': int(inputs['input_ids'].shape[1]),
                'generated_tokens': int(outputs[0].shape[0] - inputs['input_ids'].shape[1]),
                'context_docs': len(retrieved_docs)
            }

        except Exception as e:
            print(f"Generation failed: {e}")
            return {
                'answer': f"I encountered an error generating a response: {str(e)}",
                'prompt_tokens': 0,
                'generated_tokens': 0,
                'context_docs': len(retrieved_docs)
            }

"""   # Gaurd Rail System  
  Implements both input and output guardrails
"""

class GuardrailSystem:
    """Implements both input and output guardrails"""

    def __init__(self, config: RAGConfig):
        self.config = config

        self.harmful_patterns = [
            r'how to (hack|break|crack|exploit)',
            r'(illegal|unlawful|criminal) (activity|acts|behavior)',
            r'(personal|private|confidential) (information|data)',
            r'(violence|harm|hurt|kill|murder)',
            r'(drugs|weapons|explosives)',
        ]

        self.low_quality_patterns = [
            r'^(.)\1{10,}',  # Repeated characters
            r'^\d+$',        # Only numbers
            r'^[^\w]*$',     # Only punctuation
        ]

        print("Guardrail system initialized")

    def validate_input(self, query: str) -> Dict[str, Union[bool, str, List[str]]]:
        issues = []

        if len(query) > self.config.max_query_length:
            issues.append(f"Query too long ({len(query)} > {self.config.max_query_length} chars)")
        if len(query.strip()) < 3:
            issues.append("Query too short")

        for pattern in self.harmful_patterns:
            if re.search(pattern, query.lower()):
                issues.append("Potentially harmful content detected")
                break

        alpha_ratio = sum(c.isalpha() for c in query) / len(query) if query else 0
        if alpha_ratio < 0.3:
            issues.append("Query appears to be gibberish")

        return {
            'is_valid': len(issues) == 0,
            'query': query,
            'issues': issues
        }

    def validate_output(self, answer: str, query: str) -> Dict[str, Union[bool, str, List[str], float]]:
        issues = []

        if len(answer.strip()) < self.config.min_answer_length:
            issues.append("Answer too short")

        for pattern in self.low_quality_patterns:
            if re.search(pattern, answer):
                issues.append("Repetitive or low-quality content detected")
                break
        # --- NEW: Financial number validation ---
        # Extract numbers like $211.9 billion, 1,234,567, etc.
        numbers = re.findall(r'\$?\d[\d,]*(?:\.\d+)?(?:\s?(million|billion|trillion))?', answer, re.IGNORECASE)
        if any(num for num in numbers):
        # Check if revenue-like query has at least one number
          if "revenue" in query.lower() and len(numbers) == 0:
              issues.append("No numerical value found in revenue-related answer")
           # Check if multiple inconsistent numbers
          if len(numbers) > 3:
              issues.append("Answer contains too many financial numbers, may be confusing")



        suspicious_patterns = [
            r'definitely|certainly|absolutely.*\$[\d,]+',
            r'(will|going to).*(\d{4}|\d{1,2}/\d{1,2}/\d{4})',
        ]
        for pattern in suspicious_patterns:
            if re.search(pattern, answer, re.IGNORECASE):
                issues.append("Potentially non-factual or overly certain statement")
                break

        query_words = set(re.findall(r'\b\w+\b', query.lower()))
        answer_words = set(re.findall(r'\b\w+\b', answer.lower()))
        overlap_ratio = len(query_words & answer_words) / len(query_words) if query_words else 0
        if overlap_ratio < 0.1:
            issues.append("Answer may not be relevant to the query")

        confidence = max(0.0, 1.0 - (len(issues) * 0.2))
        return {
            'is_valid': len(issues) == 0,
            'answer': answer,
            'issues': issues,
            'confidence': confidence
        }

"""# MAIN RAG PIPELINE
   Main RAG pipeline orchestrating all components

"""

class CompleteRAGPipeline:
    """Main RAG pipeline orchestrating all components"""

    def __init__(self, config: Optional[RAGConfig] = None):
        self.config = config or RAGConfig()

        print("Initializing Complete RAG Pipeline")
        print("=" * 50)

        self.preprocessor = QueryPreprocessor()
        self.retriever = MultiStageRetriever(self.config)
        self.generator = ResponseGenerator(self.config)
        self.guardrails = GuardrailSystem(self.config)

        self.is_initialized = False
        self.stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'guardrail_violations': 0,
            'avg_response_time': 0.0
        }

        print("RAG Pipeline initialized successfully")

    def load_documents(self, documents: List[str], metadata: Optional[List[Dict]] = None):
        """Load, clean, chunk, and index documents"""
        print(f"Loading {len(documents)} raw documents...")
        flat_docs = []
        enriched_metadata = []

        for i, doc in enumerate(documents):
           if 'text' in doc:
              flat_docs.append(doc['text'])
              enriched_metadata.append(doc.get('metadata', {'id': i, 'type': 'paragraph', 'preview': doc['text'][:100]}))
           else:
              # Table row dict -> readable string
              doc_text = ' | '.join(f"{k}: {v}" for k, v in doc.items() if v)
              flat_docs.append(doc_text)
              enriched_metadata.append({'id': i, 'type': 'table_row', 'source': doc})

        #docs = clean_documents(documents)

        docs = clean_documents(flat_docs)

        # Chunking
        chunked_docs = []
        for d in docs:
            chunks = _chunk_text(d, self.config.chunk_size, self.config.chunk_overlap)
            chunked_docs.extend(chunks)

        print(f"Indexing {len(chunked_docs)} chunks...")
        self.retriever.build_indices(chunked_docs, metadata=None)
        self.is_initialized = True
        print("Documents loaded and indexed")

    def query(self, user_query: str) -> Dict:
        """Process a complete query through the RAG pipeline"""

        if not self.is_initialized:
            return {
                'success': False,
                'error': "Pipeline not initialized. Load documents first.",
                'answer': '',
                'confidence': 0.0,
                'response_time': 0.0
            }

        start_time = time.time()
        try:
            self.stats['total_queries'] += 1

            if self.config.enable_input_guardrails:
                input_validation = self.guardrails.validate_input(user_query)
                if not input_validation['is_valid']:
                    self.stats['guardrail_violations'] += 1
                    return {
                        'success': False,
                        'error': f"Input validation failed: {', '.join(input_validation['issues'])}",
                        'answer': '',
                        'confidence': 0.0,
                        'response_time': time.time() - start_time
                    }

            preprocessed = self.preprocessor.preprocess(user_query)

            retrieved_docs = self.retriever.multi_stage_retrieve(preprocessed['cleaned'])

            generation_result = self.generator.generate_response(preprocessed['cleaned'], retrieved_docs)

            if self.config.enable_output_guardrails:
                output_validation = self.guardrails.validate_output(
                    generation_result['answer'],
                    user_query
                )
                if not output_validation['is_valid']:
                    self.stats['guardrail_violations'] += 1
                    modified_answer = f"{generation_result['answer']}\n\n[Guardrail flags: {', '.join(output_validation['issues'])}]"
                    confidence = output_validation['confidence']
                else:
                    modified_answer = generation_result['answer']
                    confidence = output_validation['confidence']
            else:
                modified_answer = generation_result['answer']
                confidence = 0.8

            response_time = time.time() - start_time
            self.stats['avg_response_time'] = (
                self.stats['avg_response_time'] * (self.stats['total_queries'] - 1) + response_time
            ) / self.stats['total_queries']
            self.stats['successful_queries'] += 1

            result = {
                'success': True,
                'answer': modified_answer,
                'confidence': confidence,
                'response_time': response_time,
                'method': 'Multi-Stage Retrieval (Hybrid + Cross-Encoder)',
                'retrieved_docs': len(retrieved_docs),
                'context_tokens': generation_result.get('prompt_tokens', 0),
                'generated_tokens': generation_result.get('generated_tokens', 0),
                'preprocessing': preprocessed,
                'retrieval_details': retrieved_docs,
                'stats': self.get_statistics()
            }
            return result

        except Exception as e:
            print(f"Pipeline error: {e}")
            return {
                'success': False,
                'error': f"Pipeline error: {str(e)}",
                'answer': '',
                'confidence': 0.0,
                'response_time': time.time() - start_time
            }

    def get_statistics(self) -> Dict:
        return {
            **self.stats,
            'success_rate': self.stats['successful_queries'] / max(self.stats['total_queries'], 1),
            'guardrail_violation_rate': self.stats['guardrail_violations'] / max(self.stats['total_queries'], 1)
        }

"""# RAG INTERFACE

'''User interface for the RAG system'''
"""

class RAGInterface:
    """User interface for the RAG system"""

    def __init__(self, pipeline: CompleteRAGPipeline):
        self.pipeline = pipeline

    def create_gradio_interface(self):
        """Create Gradio interface with file upload, indexing, and querying"""

        def index_documents(files, chunk_size, chunk_overlap, enable_guardrails):
            if not files or len(files) == 0:
                return "No files provided.", "0", "0"
            paths = [Path(f.name) for f in files]
            docs = load_files_to_strings(paths)
            if not docs:
                return "No readable documents found.", "0", "0"

            # Apply settings
            self.pipeline.config.chunk_size = int(chunk_size)
            self.pipeline.config.chunk_overlap = int(chunk_overlap)
            self.pipeline.config.enable_input_guardrails = bool(enable_guardrails)
            self.pipeline.config.enable_output_guardrails = bool(enable_guardrails)

            self.pipeline.load_documents(docs)
            total_chunks = len(self.pipeline.retriever.documents)
            return f"Indexed {len(docs)} files into {total_chunks} chunks.", str(len(docs)), str(total_chunks)

        def process_query(query, enable_guardrails, max_docs):
            if not query or not query.strip():
                return "Please enter a query.", 0.0, "No method", "0.00s", "No details"

            original_guardrails_in = self.pipeline.config.enable_input_guardrails
            original_guardrails_out = self.pipeline.config.enable_output_guardrails
            original_k = self.pipeline.config.final_retrieval_k

            self.pipeline.config.enable_input_guardrails = bool(enable_guardrails)
            self.pipeline.config.enable_output_guardrails = bool(enable_guardrails)
            self.pipeline.config.final_retrieval_k = int(max_docs)

            try:
                result = self.pipeline.query(query)
                if result['success']:
                    details = f"""
Pipeline Details:
- Method: {result['method']}
- Retrieved documents: {result['retrieved_docs']}
- Context tokens: {result['context_tokens']}
- Generated tokens: {result['generated_tokens']}
- Response time: {result['response_time']:.3f}s

Query Preprocessing:
- Original: {result['preprocessing']['original']}
- Cleaned: {result['preprocessing']['cleaned']}
- No stopwords: {result['preprocessing']['no_stopwords']}

System Statistics:
- Total queries: {result['stats']['total_queries']}
- Success rate: {result['stats']['success_rate']:.2%}
- Avg response time: {result['stats']['avg_response_time']:.3f}s

Top Retrieved Documents:
"""
                    for i, doc in enumerate(result['retrieval_details'][:min(3, len(result['retrieval_details']))]):
                        score = doc.get('cross_encoder_score', doc.get('hybrid_score', 0.0))
                        snippet = (doc['text'][:200] + '...') if len(doc['text']) > 200 else doc['text']
                        details += f"\nDocument {i+1} (Score: {score:.3f}):\n{snippet}\n"

                    return (
                        result['answer'],
                        float(result['confidence']),
                        result['method'],
                        f"{result['response_time']:.2f}s",
                        details
                    )
                else:
                    return result.get('error', 'Unknown error'), 0.0, "Error", f"{result.get('response_time', 0.0):.2f}s", str(result)
            finally:
                self.pipeline.config.enable_input_guardrails = original_guardrails_in
                self.pipeline.config.enable_output_guardrails = original_guardrails_out
                self.pipeline.config.final_retrieval_k = original_k

        with gr.Blocks(title="Advanced RAG System") as interface:
            gr.Markdown("# Advanced RAG System with Multi-Stage Retrieval")
            gr.Markdown(
                "This system uses Multi-Stage Retrieval: "
                "Stage 1 hybrid search (Dense + BM25), Stage 2 cross-encoder re-ranking. "
                "Includes guardrails and performance monitoring."
            )

            with gr.Row():
                with gr.Column(scale=2):
                    gr.Markdown("## Index Documents")
                    files = gr.File(label="Upload .txt or .docx or .pdf files", file_count="multiple", type="filepath")
                    with gr.Row():
                        chunk_size = gr.Number(label="Chunk Size (words)", value=300, precision=0)
                        chunk_overlap = gr.Number(label="Chunk Overlap (words)", value=50, precision=0)
                    enable_guardrails_idx = gr.Checkbox(label="Enable Guardrails", value=True)
                    index_btn = gr.Button("Index Documents")
                    index_status = gr.Textbox(label="Indexing Status")
                    num_files = gr.Textbox(label="Files Indexed", interactive=False)
                    num_chunks = gr.Textbox(label="Chunks Indexed", interactive=False)

                with gr.Column(scale=2):
                    gr.Markdown("## Ask a Question")
                    query_input = gr.Textbox(
                        label="Your Question",
                        placeholder="Ask about the loaded documents...",
                        lines=4
                    )
                    with gr.Row():
                        enable_guardrails_q = gr.Checkbox(label="Enable Guardrails", value=True)
                        max_docs = gr.Slider(minimum=1, maximum=10, value=5, step=1, label="Max Retrieved Documents")
                    submit_btn = gr.Button("Ask")

                    answer_output = gr.Textbox(label="Answer", lines=8)
                    with gr.Row():
                        confidence_output = gr.Number(label="Confidence", precision=2)
                        method_output = gr.Textbox(label="Method Used")
                        time_output = gr.Textbox(label="Response Time")
                    with gr.Accordion("Detailed Information", open=False):
                        details_output = gr.Markdown(label="Pipeline Details")

            gr.Examples(
                examples=[
                    ["Summarize the main points about revenue recognition policies."],
                    ["What are the risk factors mentioned in the documents?"],
                    ["Explain the change in accounting estimate described."]
                ],
                inputs=query_input
            )

            index_btn.click(
                fn=index_documents,
                inputs=[files, chunk_size, chunk_overlap, enable_guardrails_idx],
                outputs=[index_status, num_files, num_chunks]
            )

            submit_btn.click(
                fn=process_query,
                inputs=[query_input, enable_guardrails_q, max_docs],
                outputs=[answer_output, confidence_output, method_output, time_output, details_output]
            )

        return interface

"""# Entry point for direct execution"""

# ===============================
# Entry point for direct execution
# ===============================

class RAGStreamLit:
    """User interface for the RAG system"""

    def __init__(self, pipeline: CompleteRAGPipeline):
        self.pipeline = pipeline

    def index_documents(files, chunk_size, chunk_overlap, enable_guardrails):
           if not files or len(files) == 0:
                return "No files provided.", "0", "0"
           paths = [Path(f.name) for f in files]
           docs = load_files_to_strings(paths)
           if not docs:
               return "No readable documents found.", "0", "0"

           # Apply settings
           self.pipeline.config.chunk_size = int(chunk_size)
           self.pipeline.config.chunk_overlap = int(chunk_overlap)
           self.pipeline.config.enable_input_guardrails = bool(enable_guardrails)
           self.pipeline.config.enable_output_guardrails = bool(enable_guardrails)

           self.pipeline.load_documents(docs)
           total_chunks = len(self.pipeline.retriever.documents)
           return f"Indexed {len(docs)} files into {total_chunks} chunks.", str(len(docs)), str(total_chunks)

#if __name__ == "__main__":
    #cfg = RAGConfig()
    #pipeline = CompleteRAGPipeline(cfg)

    #ui = RAGInterface(pipeline)
    #app = ui.create_gradio_interface()
    #app.launch(share=True)   # share=False if running locally
    #app.launch(share=True, inline=False, inbrowser=True, debug=True)
