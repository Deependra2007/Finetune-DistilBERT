# -*- coding: utf-8 -*-
"""ConvAI_Group_111_RAG_vs_FT_V02.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19Ai4lpiBH42rXqSWEeVCZLm16YPwfnjC

# Assignment 2 â€“ Comparative Financial QA System: RAG vs Fine-Tuning

## Objective
Develop and compare two systems for answering questions based on company financial statements (last two years):

- **Retrieval-Augmented Generation (RAG) Chatbot**: Combines document retrieval and generative response.  
- **Fine-Tuned Language Model (FT) Chatbot**: Directly fine-tunes a small open-source language model on financial Q&A.  

Use the same financial data for both methods and perform a detailed comparison on **accuracy, speed, and robustness**.

---

## Step-by-Step Tasks

### 1. Data Collection & Preprocessing
1. Obtain financial statements for the last two years (publicly available or from a group memberâ€™s company).  
2. Convert documents (PDF, Excel, HTML) to plain text using OCR or appropriate parsers.  
3. Clean text by removing noise like headers, footers, and page numbers.  
4. Segment reports into logical sections (e.g., income statement, balance sheet).  
5. Construct at least **50 Q/A pairs** reflecting the financial data.  

**Example:**  
- Q: *What was the companyâ€™s revenue in 2023?*  
- A: *The companyâ€™s revenue in 2023 was $4.13 billion.*  

---

### 2. Retrieval-Augmented Generation (RAG) System Implementation

#### 2.1 Data Processing
- Split the cleaned text into chunks (â‰¥2 sizes, e.g., 100 and 400 tokens).  
- Assign unique IDs and metadata to chunks.  

#### 2.2 Embedding & Indexing
- Embed chunks using a small open-source sentence embedding model (e.g., `all-MiniLM-L6-v2`, `E5-small-v2`).  
- Build:
  - Dense vector store (e.g., FAISS, ChromaDB).  
  - Sparse index (BM25 or TF-IDF).  

#### 2.3 Hybrid Retrieval Pipeline
For each user query:
1. Preprocess (clean, lowercase, stopword removal).  
2. Generate query embedding.  
3. Retrieve top-N chunks from:
   - Dense retrieval (vector similarity).  
   - Sparse retrieval (BM25).  
4. Combine results (union or weighted fusion).  

#### 2.4 Advanced RAG Technique (Select One)

| Remainder (Group# mod 5) | Advanced Technique              | Description |
|---------------------------|----------------------------------|-------------|
| 1 | Multi-Stage Retrieval | Stage 1: Broad retrieval; Stage 2: Re-rank with cross-encoder. |
| 2 | Chunk Merging & Adaptive Retrieval | Merge adjacent chunks/adapt size based on query. |
| 3 | Re-Ranking with Cross-Encoders | Re-rank retrieved chunks by query relevance. |
| 4 | Hybrid Search | Combine BM25 + dense retrieval for balanced recall/precision. |
| 0 | Memory-Augmented Retrieval | Add persistent memory of frequent/important Q&A. |

#### 2.5 Response Generation
- Use a small open-source generative model (e.g., `DistilGPT2`, `GPT-2 Small`, `Llama-2 7B`).  
- Input = retrieved passages + query.  
- Limit tokens to model context window.  

#### 2.6 Guardrail Implementation
- **Input-side**: Validate queries (filter irrelevant/harmful).  
- **Output-side**: Filter hallucinated/non-factual outputs.  

#### 2.7 Interface Development
Build UI (Streamlit, Gradio, CLI, GUI) with features:
- Accept query  
- Display: answer, confidence, method, response time  
- Switch between RAG and FT modes  

---

### 3. Fine-Tuned Model System Implementation

#### 3.1 Q/A Dataset Preparation
- Use the same ~50 Q/A pairs.  
- Convert into fine-tuning dataset format.  

#### 3.2 Model Selection
- Choose a small open-source model (e.g., DistilBERT, MiniLM, GPT-2 Small/Medium, Llama-2 7B, Falcon 7B, Mistral 7B).  
- **No closed/proprietary APIs.**  

#### 3.3 Baseline Benchmarking
- Evaluate pre-trained model on â‰¥10 test questions.  
- Record accuracy, confidence, and inference speed.  

#### 3.4 Fine-Tuning
- Fine-tune model on Q/A dataset.  
- Log hyperparameters: learning rate, batch size, epochs, compute setup.  
- Use assigned efficient fine-tuning method.  

#### 3.5 Advanced Fine-Tuning Technique (Select One)

| Remainder (Group# mod 5) | Technique | Description |
|---------------------------|-----------|-------------|
| 1 | Supervised Instruction Fine-Tuning | Train on instruction-style Q/A. |
| 2 | Adapter-Based Tuning | Insert adapter modules for efficiency. |
| 3 | Mixture-of-Experts Fine-Tuning | Multi-expert architectures for tuning/inference. |
| 4 | Retrieval-Augmented Fine-Tuning | Combine retrieval with fine-tuning. |
| 0 | Continual Learning / Domain Adaptation | Incrementally adapt to new financial data. |

#### 3.6 Guardrail Implementation
- Input/output guardrails (similar to RAG).  

#### 3.7 Interface Development
- Integrate FT model into same UI.  
- Display: answer, confidence, method, inference time.  
- Allow switching between methods.  

---

### 4. Testing, Evaluation & Comparison

#### 4.1 Test Questions (Mandatory)
- **Relevant, high-confidence**: Clear fact in data.  
- **Relevant, low-confidence**: Ambiguous/sparse info.  
- **Irrelevant**: e.g., *What is the capital of France?*  

#### 4.2 Extended Evaluation
- Evaluate â‰¥10 financial questions.  
- Record for each system:
  - Real answer  
  - Model answer  
  - Confidence  
  - Response time  
  - Correctness (Y/N)  

#### 4.3 Results Table (Example)

| Question | Method | Answer | Confidence | Time (s) | Correct (Y/N) |
|----------|--------|--------|------------|----------|---------------|
| Revenue in 2023? | RAG | $4.02B | 0.92 | 0.50 | Y |
| Revenue in 2023? | Fine-Tune | $4.13B | 0.93 | 0.41 | Y |
| Unique products? | RAG | 13,000 units | 0.81 | 0.79 | Y |
| Unique products? | Fine-Tune | 13,240 units | 0.89 | 0.65 | Y |
| Capital of France? | RAG | Data not in scope | 0.35 | 0.46 | Y |
| Capital of France? | Fine-Tune | Not applicable | 0.85 | 0.38 | Y |

#### 4.4 Analysis
- Compare average inference speed and accuracy.  
- Discuss:
  - Strengths of RAG (adaptability, factual grounding).  
  - Strengths of FT (fluency, efficiency).  
  - Robustness to irrelevant queries.  
  - Practical trade-offs.  

---

### 5. Submission Requirements
Submit a **ZIP file per group**: `Group_<Number>_RAG_vs_FT.zip`

**Contents:**
1. Python Notebook (.ipynb/.py) with:
   - Data processing  
   - RAG + FT implementations  
   - Advanced technique sections  
   - Testing & comparison tables  
2. PDF Report with:
   - 3 screenshots (queries, answers, confidence, time, method)  
   - Summary comparison table  
   - Hosted demo app link  
3. Use **open-source only** (no proprietary APIs).  
4. Comment & document all code clearly.  

---

## Notes & Recommendations
- Use free/institutional GPU resources (Colab, Kaggle, campus clusters).  
- Quantitative comparison + documentation are critical.  
- Implement **guardrails** (mandatory).  
- UI should be user-friendly and show which method is used.  

---

ðŸ’¡ **Goal**: Hands-on experience in building & comparing RAG vs Fine-Tuning for specialized **financial Q&A** using open-source technologies.  
Good luck!

===============================================================================
RAG-Based QA System with Guardrails (Colab Notebook)
===============================================================================

# Install and Import Libraries
"""

import os
import re
import time
import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Optional, Tuple, Union
from dataclasses import dataclass, asdict
import warnings
warnings.filterwarnings("ignore")

# Core ML libraries
import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM
)
from sentence_transformers import SentenceTransformer, CrossEncoder
import faiss
from rank_bm25 import BM25Okapi

# Text processing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import string

# Optional .docx support
try:
    from docx import Document as DocxDocument
    HAS_DOCX = True
except Exception:
    HAS_DOCX = False

try:
    import pdfplumber
    HAS_PDFPLUMBER = True
except ImportError:
    HAS_PDFPLUMBER = False

# Download required NLTK data (idempotent)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords', quiet=True)


def _read_txt(path: Path) -> str:
    try:
        return path.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return path.read_text(encoding="latin-1", errors="ignore")



def _read_docx(path: Path) -> Tuple[str, List[Dict]]:
    """
    Extract text and structured tables from a .docx file.
    Returns:
        full_text (str): flattened text for embeddings
        tables_json (List[Dict]): structured representation of tables
    """
    if not HAS_DOCX:
        raise RuntimeError("python-docx is not installed. Please install with: pip install python-docx")

    doc = DocxDocument(str(path))

    # --- Extract paragraphs ---
    paras = [p.text for p in doc.paragraphs if p.text and p.text.strip()]
    full_text = "\n".join(paras)

    # --- Extract tables ---
    tables_json = []
    for table_idx, table in enumerate(doc.tables):
        table_data = []
        for row in table.rows:
            row_data = [cell.text.strip() for cell in row.cells]
            table_data.append(row_data)

        # Convert rows to dict using first row as headers
        if table_data:
            headers = table_data[0]
            for row_num, row in enumerate(table_data[1:], start=1):
                row_json = {headers[i]: row[i] for i in range(min(len(headers), len(row)))}
                row_json.update({'table_index': table_idx, 'row_index': row_num, 'type': 'table_row', 'source': row})
                tables_json.append(row_json)

    return full_text, tables_json





def _read_pdf(path: Path) -> Tuple[str, List[Dict]]:
    """
    Extract text and tables from PDF.
    Returns:
        full_text (str)
        tables_json (List[Dict])
    """
    if not HAS_PDFPLUMBER:
        raise RuntimeError("pdfplumber is not installed. Please install with: pip install pdfplumber")

    text = []
    tables_json = []

    with pdfplumber.open(str(path)) as pdf:
        for page in pdf.pages:
            # Extract text
            content = page.extract_text()
            if content:
                text.append(content.strip())

            # Extract tables
            tables = page.extract_tables()
            for tbl in tables:
                if not tbl:
                    continue
                headers = tbl[0]
                for row in tbl[1:]:
                    row_json = {headers[i]: row[i] for i in range(min(len(headers), len(row)))}
                    tables_json.append(row_json)

    return "\n".join(text), tables_json

def load_files_to_strings(paths: List[Path]) -> List[Dict]:
    docs = []
    for p in paths:
        suffix = p.suffix.lower()
        try:
            if suffix == ".txt":
                text = _read_txt(p)
                docs.append({"text": text, "metadata": {"source": str(p)}, "score": 0.0})
            elif suffix == ".docx":
                full_text, tables_json = _read_docx(p)
                docs.append({"text": full_text, "metadata": {"source": str(p)}, "score": 0.0})
                # Include table rows as separate entries
                for row in tables_json:
                    row_text = ' | '.join(f'{k}: {v}' for k, v in row.items() if k not in ['table_index', 'row_index', 'type', 'source'])
                    docs.append({"text": row_text, "metadata": {"source": str(p)}, "score": 0.0})
            elif suffix == ".pdf":
                text = _read_pdf(p)
                docs.append({"text": text, "metadata": {"source": str(p)}, "score": 0.0})
                for row in tables:  # optional: index table rows
                   row_text = ' | '.join(f'{k}: {v}' for k, v in row.items() if v is not None)
                   docs.append({"text": row_text, "metadata": {"source": str(p)}, "score": 0.0})
            else:
                pass
        except Exception as e:
            print(f"Failed to read {p}: {e}")
    return docs


def extract_numeric_answer(query: str, retrieved_docs: List[Dict]) -> Optional[Tuple[str, str]]:
    """
    Tiered extractor for financial values:
    - If query is about revenue â†’ extract revenue.
    - If query is about profit/net income/earnings â†’ extract profit.
    Returns (value, source_text) or None
    """

    query_lower = query.lower()
    is_revenue = "revenue" in query_lower
    is_profit = any(x in query_lower for x in ["net income", "net profit", "earnings"])

    if not (is_revenue or is_profit):
        return None, None

    # Detect year (e.g. FY 2023)
    target_year = None
    m = re.search(r"(20\d{2})", query_lower)
    if m:
        target_year = m.group(1)

    # --------------------
    # Revenue patterns
    # --------------------
    revenue_strict = re.compile(
        r"(?:total\s+|consolidated\s+)?revenue[^$\d]{0,20}([\$â‚¬â‚¹]?\s?\d[\d,]*(?:\.\d+)?)(?:\s?(million|billion|trillion))?",
        re.IGNORECASE
    )
    revenue_table = (
        re.compile(rf"Revenue\s*\|\s*{target_year}:\s*\$?\s*([\d,]+)", re.IGNORECASE)
        if target_year else None
    )
    revenue_flex = re.compile(
        rf"(?:total\s+)?revenue[^0-9$]{{0,15}}[:\|]?\s*\$?\s*([\d,]+)",
        re.IGNORECASE
    )

    # --------------------
    # Profit patterns
    # --------------------
    profit_strict = re.compile(
        r"(?:net\s+income|net\s+profit|earnings)[^$\d]{0,20}([\$â‚¬â‚¹]?\s?\d[\d,]*(?:\.\d+)?)(?:\s?(million|billion|trillion))?",
        re.IGNORECASE
    )
    profit_table = (
        re.compile(rf"(?:Net\s+Income|Earnings)\s*\|\s*{target_year}:\s*\$?\s*([\d,]+)", re.IGNORECASE)
        if target_year else None
    )

    # --------------------
    # Loop through docs
    # --------------------
    for doc in retrieved_docs:
        text = doc.get("text", "")

        if is_revenue:
            match = revenue_flex.search(text) or revenue_strict.search(text)
            if match:
                norm = _normalize_value(match, text)
                if norm:
                    print("[DEBUG] extract_numeric_answer 1 ",norm, text[:200])
                    return norm, text[:200]

            if revenue_table:
                match = revenue_table.search(text)
                if match:
                    raw = match.group(1).replace(",", "")
                    try:
                        val = int(raw)
                        return f"${val/1e3:,.1f} billion", text[:200]  # assume "in millions"
                    except:
                        continue

        if is_profit:
            match = profit_strict.search(text)
            if match:
                norm = _normalize_value(match, text)
                if norm:
                    print("[DEBUG] extract_numeric_answer 2 ",norm, text[:200])
                    return norm, text[:200]

            if profit_table:
                match = profit_table.search(text)
                if match:
                    raw = match.group(1).replace(",", "")
                    try:
                        val = int(raw)
                        return f"${val/1e3:,.1f} billion", text[:200]  # assume "in millions"
                    except:
                        continue

    return None, None


def _normalize_value(match, text: str) -> Optional[str]:
    """Helper: normalize numeric with units or implicit context."""
    number, unit = match.groups()
    number_clean = number.replace(",", "").replace("$", "").strip()

    # Skip false year matches
    if re.fullmatch(r"19\d{2}|20\d{2}", number_clean):
        print("[DEBUG] _normalize_value 1 ")
        return None

    try:
        value = float(number_clean)
        if unit:
            unit = unit.lower()
            if unit == "million":
                value *= 1e6
            elif unit == "billion":
                value *= 1e9
            elif unit == "trillion":
                value *= 1e12
        elif re.search(r"in\s+millions", text, re.IGNORECASE):
            value *= 1e6
        elif re.search(r"in\s+billions", text, re.IGNORECASE):
            value *= 1e9

        # Pretty formatting
        if value >= 1e9:
            return f"${value/1e9:,.1f} billion"
        elif value >= 1e6:
            return f"${value/1e6:,.1f} million"
        else:
            return f"${value:,.0f}"
    except:
        print("[DEBUG] _normalize_value 2 ")
        return None

"""#  Data Processing"""

def _chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """Simple word-wise chunking with overlap."""
    words = text.split()
    if chunk_size <= 0:
        return [text]
    chunks = []
    start = 0
    step = max(1, chunk_size - chunk_overlap)
    while start < len(words):
        end = min(len(words), start + chunk_size)
        chunks.append(' '.join(words[start:end]))
        if end == len(words):
            break
        start += step
    return chunks


def _normalize_space(s: str) -> str:
    return re.sub(r'\s+', ' ', s).strip()


def _remove_repeated_phrases(text: str) -> str:
    """
    Remove common repeated artefacts and collapse long repetitions.
    Example: 'change in accounting estimate' repeated many times.
    """
    patterns = [
        r'(change in accounting estimate[\.\s,;:]*){2,}',
        r'(forward-looking statements[\.\s,;:]*){2,}',
        r'(safe harbor statement[\.\s,;:]*){2,}',
    ]
    out = text
    for p in patterns:
        out = re.sub(p, lambda m: _normalize_space(m.group(0).split()[0:4] and " ".join(m.group(0).split()[0:4])), out, flags=re.IGNORECASE)
    # Collapse repeated lines
    lines = [ln.strip() for ln in out.splitlines() if ln.strip()]
    deduped = []
    seen = set()
    for ln in lines:
        norm = _normalize_space(ln.lower())
        if norm in seen:
            continue
        seen.add(norm)
        deduped.append(ln)
    return _normalize_space("\n".join(deduped))


def clean_documents(docs: List[str]) -> List[str]:
    cleaned = []
    seen = set()
    for doc in docs:
        text = _normalize_space(doc)
        text = _remove_repeated_phrases(text)
        if not text:
            continue
        if text in seen:
            continue
        seen.add(text)
        cleaned.append(text)
    return cleaned

"""# RAG CONSTRUCTION

RAG configuration File all the detals of constants used
"""

@dataclass
class RAGConfig:
    """Configuration for the complete RAG pipeline"""
    # Embedding settings
    embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    cross_encoder_model: str = "cross-encoder/ms-marco-MiniLM-L-6-v2"

    # Generation settings
    generator_model: str = "distilgpt2"  # or "gpt2" or "microsoft/DialoGPT-small"
    max_context_length: int = 512
    max_generation_length: int = 150

    # Retrieval settings
    initial_retrieval_k: int = 20  # Stage 1: broad retrieval
    final_retrieval_k: int = 5     # Stage 2: after re-ranking
    hybrid_alpha: float = 0.6      # Dense vs sparse weight

    # Processing settings
    chunk_size: int = 300
    chunk_overlap: int = 50
    batch_size: int = 16

    # Guardrail settings
    enable_input_guardrails: bool = True
    enable_output_guardrails: bool = True
    max_query_length: int = 500
    min_answer_length: int = 10

    # Interface settings
    interface_type: str = "gradio"  # "gradio" or "streamlit"

"""# Query Processor"""

class QueryPreprocessor:
    """Handles query preprocessing with multiple cleaning strategies"""

    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        # Add custom domain-specific stop words
        self.stop_words.update(['would', 'could', 'should', 'might', 'may'])

    def clean_query(self, query: str) -> str:
        """Basic cleaning: remove special chars, normalize whitespace"""
        query = re.sub(r'[^\w\s\?\.\!,]', ' ', query)
        query = re.sub(r'\s+', ' ', query).strip()
        return query

    def remove_stopwords(self, query: str) -> str:
        """Remove stopwords while preserving question structure"""
        tokens = word_tokenize(query.lower())
        question_words = {'what', 'when', 'where', 'who', 'why', 'how', 'which'}

        filtered_tokens = []
        for token in tokens:
            if token not in self.stop_words or token in question_words:
                if token not in string.punctuation:
                    filtered_tokens.append(token)

        return ' '.join(filtered_tokens)

    def expand_query(self, query: str) -> List[str]:
        """Generate query variations for better retrieval"""
        variations = [query]

        cleaned = self.clean_query(query)
        if cleaned != query:
            variations.append(cleaned)

        no_stopwords = self.remove_stopwords(query)
        if no_stopwords and no_stopwords != query.lower():
            variations.append(no_stopwords)

        return variations

    def preprocess(self, query: str) -> Dict[str, Union[str, List[str]]]:
        """Complete preprocessing pipeline"""
        return {
            'original': query,
            'cleaned': self.clean_query(query),
            'no_stopwords': self.remove_stopwords(query),
            'variations': self.expand_query(query)
        }

"""   # Multi Stage Retrieval
    
    Advanced RAG Technique: Multi-Stage Retrieval
    Stage 1: Broad retrieval using hybrid search
    Stage 2: Precise re-ranking using cross-encoder
"""

class MultiStageRetriever:
    """
    Advanced RAG Technique: Multi-Stage Retrieval
    Stage 1: Broad retrieval using hybrid search
    Stage 2: Precise re-ranking using cross-encoder
    """

    def __init__(self, config: RAGConfig):
        self.config = config

        print("Loading retrieval models...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.embedding_model = SentenceTransformer(config.embedding_model, device=device)
        self.cross_encoder = CrossEncoder(config.cross_encoder_model, device=device)

        self.dense_index = None
        self.sparse_index = None
        self.documents: List[str] = []
        self.metadata: List[Dict] = []
        self.scores = []

        print("Multi-stage retriever initialized")

    def build_indices(self, documents: List[str], metadata: Optional[List[Dict]] = None):
        """Build both dense and sparse indices"""
        print(f"Building indices for {len(documents)} documents...")

        # Normalize documents -> always text
        flat_docs = []
        enriched_metadata = []

        for i, doc in enumerate(documents):
          if isinstance(doc, dict):
             if 'text' in doc:
                flat_docs.append(doc['text'])
                meta=doc.get('metadata', {'id': i, 'type': 'paragraph', 'preview': doc['text'][:100]})
             else:
                doc_text = ' | '.join(f"{k}: {v}" for k, v in doc.items() if v)
                flat_docs.append(doc_text)
                meta={'id': i, 'type': 'table_row', 'source': doc}
          else:
            flat_docs.append(doc)
            meta={'id': i, 'type': 'paragraph', 'preview': str(doc)[:100]}


          meta['score'] = 0.0
          enriched_metadata.append(meta)

        self.documents = flat_docs
        self.metadata = metadata if metadata is not None else enriched_metadata
        # Initialize default score for each document
        self.scores = [0.0 for _ in self.documents]



        print("Creating embeddings...")
         # Dense index (FAISS)
        embeddings = self.embedding_model.encode(
            self.documents,
            batch_size=self.config.batch_size,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True,  # cosine via inner product
        ).astype(np.float32)

        dimension = embeddings.shape[1]
        self.dense_index = faiss.IndexFlatIP(dimension)
        self.dense_index.add(embeddings)

        # Sparse index (BM25)
        print("Building BM25 index...")
        tokenized_docs = [doc.lower().split() for doc in self.documents]
        self.sparse_index = BM25Okapi(tokenized_docs)

        print(f"Indices built: {len(documents)} documents indexed")
        return len(documents)
    

    def _revenue_boost(self, query: str, docs: List[Dict]) -> List[Dict]:
        """Boost docs mentioning revenue + numbers when query relates to revenue/financials"""
        if not re.search(r"\brevenue|income|sales|earnings\b", query.lower()):
            return docs  # no boost needed

        boosted = []
        for d in docs:

            base_score = d.get('cross_encoder_score')
            if base_score is None:
               base_score = d.get('hybrid_score')
            if base_score is None:
               base_score = d.get('score', 0.0)

            score = float(base_score)


            text = d["text"]

            # Look for revenue + number in same chunk
            if re.search(r"\brevenue\b", text, re.IGNORECASE) and re.search(r"[\$â‚¬â‚¹]?\s?\d[\d,\.]+\s?(million|billion|trillion)?", text, re.IGNORECASE):
                score += 5.0  # strong boost


            # --- Case 2: Table-style entries (e.g. "Total revenue .... 211,000,000") ---
            elif re.search(r"(total\s+)?revenue", text, re.IGNORECASE) and re.search(r"\d{3,}(?:,\d{3})*(?:\.\d+)?",text, re.IGNORECASE):
                score += 4.0


            # Slight bump for any financial keyword
            elif re.search(r"\b(income|profit|sales|total)\b", text, re.IGNORECASE):
                score += 2.0

            d['score'] = score  # ensure score is always set
            boosted.append({**d, "score": score})

        # Re-sort by boosted score
        boosted = sorted(boosted, key=lambda x: x["score"], reverse=True)

        print("[DEBUG] Revenue Boosted documents ",boosted)
        return boosted

    def stage1_broad_retrieval(self, query: str, k: Optional[int] = None) -> List[Dict]:
        k = k or self.config.initial_retrieval_k

        query_embedding = self.embedding_model.encode(
               [query],
               convert_to_numpy=True,
               normalize_embeddings=True
               ).astype(np.float32)

        print("[DEBUG] Query:", query)
        print("[DEBUG] Query embedding norm:", np.linalg.norm(query_embedding))

        dense_scores, dense_indices = self.dense_index.search(
            query_embedding,
            min(k, len(self.documents))
            )

        print("[DEBUG] Dense scores:", dense_scores)
        print("[DEBUG] Dense indices:", dense_indices)

        dense_results = []

        for score, idx in zip(dense_scores[0], dense_indices[0]):
            if 0 <= idx < len(self.documents):
                dense_results.append({
                    'index': int(idx),
                    'dense_score': float(score),
                    'sparse_score': 0.0,
                    'text': self.documents[idx],
                    'metadata': self.metadata[idx]
                })


        # Sparse retrieval (BM25)
        query_tokens = query.lower().split()
        sparse_scores = self.sparse_index.get_scores(query_tokens)


        print("[DEBUG] Query tokens:", query_tokens)
        print("[DEBUG] Sparse scores (first 10):", sparse_scores[:10] if sparse_scores is not None else None)

        if sparse_scores is None or len(sparse_scores) == 0:
            sparse_top_indices = []
        else:
            sparse_top_indices = np.argsort(sparse_scores)[::-1][:k]

        alpha = self.config.hybrid_alpha
        combined_results: Dict[int, Dict] = {}

        for result in dense_results:
            combined_results[result['index']] = result

        for idx in sparse_top_indices:
            sparse_score = float(sparse_scores[idx])
            if idx in combined_results:
                combined_results[idx]['sparse_score'] = sparse_score
            else:
                combined_results[idx] = {
                    'index': int(idx),
                    'dense_score': 0.0,
                    'sparse_score': sparse_score,
                    'text': self.documents[idx],
                    'metadata': self.metadata[idx]
                }

        # Normalize sparse scores
        max_sparse = float(max(sparse_scores)) if isinstance(sparse_scores, (list, np.ndarray)) and len(sparse_scores) > 0 else 1.0
        max_sparse = max(max_sparse, 1e-8)

        for r in combined_results.values():
            dense_norm = r['dense_score']  # already cosine-ish
            sparse_norm = r['sparse_score'] / max_sparse
            r['hybrid_score'] = alpha * dense_norm + (1 - alpha) * sparse_norm
            r['score'] = r['hybrid_score']
            r['stage'] = 'broad_retrieval'

        sorted_results = sorted(combined_results.values(), key=lambda x: x['hybrid_score'], reverse=True)
        print("[DEBUG] Combined results (top 5):", sorted_results[:5])


        return sorted_results[:k]

    def stage2_precise_reranking(self, query: str, candidates: List[Dict], k: Optional[int] = None) -> List[Dict]:
        k = k or self.config.final_retrieval_k
        if not candidates:
            return []

        print(f"Re-ranking {len(candidates)} candidates with cross-encoder...")
        query_doc_pairs = []
        for candidate in candidates:
            doc_text = candidate['text'][:500]
            query_doc_pairs.append([query, doc_text])

        try:
            cross_encoder_scores = self.cross_encoder.predict(query_doc_pairs)
            for i, candidate in enumerate(candidates):
                candidate['cross_encoder_score'] = float(cross_encoder_scores[i])
                candidate['score'] = candidate['cross_encoder_score']
                candidate['stage'] = 'precise_reranking'
            reranked = sorted(candidates, key=lambda x: x['cross_encoder_score'], reverse=True)
            print("[DEBUG] Cross Encoder Re-Ranked documents ",reranked[:k])
            return reranked[:k]

        except Exception as e:
            print(f"Cross-encoder re-ranking failed: {e}")
            return candidates[:k]

    def multi_stage_retrieve(self, query: str) -> List[Dict]:

        # Stage 1: broad retrieval
        start_time = time.time()
        stage1_results = self.stage1_broad_retrieval(query, self.config.initial_retrieval_k)
        stage1_time = time.time()

        # Stage 2: precise reranking
        final_results = self.stage2_precise_reranking(query, stage1_results, self.config.final_retrieval_k)
        stage2_time = time.time()

        # Stage 3: financial boosting
        final_results = self._revenue_boost(query, final_results)

        enriched_results = []
        for i, result in enumerate(final_results):
            meta = result.get("metadata", {})

            enriched = {
            "final_rank": i + 1,
            "stage1_time": stage1_time - start_time,
            "stage2_time": stage2_time - stage1_time,
            "total_time": stage2_time - start_time,
            #"score": result.get("cross_encoder_score", result.get("hybrid_score", 0.0)),
            "score": result.get("score", 0.0),
            "text": result["text"],
            "metadata": meta
             }
            print("[DEBUG] Final scores ",enriched["score"])
            if meta.get("type") == "table_row":
               enriched["table_row"] = meta.get("source")

            enriched_results.append(enriched)


        print(f"Multi-stage retrieval completed: {len(enriched_results)} results")


        return enriched_results

"""   # ResponseGenerator  
  
  Handles response generation with context management
"""

class ResponseGenerator:
    """Handles response generation with context management"""
    def __init__(self, config: RAGConfig):
        self.config = config

        print(f"Loading generation model: {config.generator_model}")
        self.tokenizer = AutoTokenizer.from_pretrained(config.generator_model)
        self.model = AutoModelForCausalLM.from_pretrained(config.generator_model)

        # Proper padding configuration for GPT2-like models
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        if getattr(self.model.config, "pad_token_id", None) is None:
            self.model.config.pad_token_id = self.tokenizer.pad_token_id

        print("Response generator initialized")

    def prepare_context(self, query: str, retrieved_docs: List[Dict]) -> str:
        context_parts = []
        current_tokens = 0
        max_context_tokens = max(64, self.config.max_context_length - 100)  # buffer for query + prompt

        for doc in retrieved_docs:
            doc_text = doc['text']
            doc_tokens = len(self.tokenizer.encode(doc_text, add_special_tokens=False))
            if current_tokens + doc_tokens <= max_context_tokens:
                context_parts.append(f"Document {len(context_parts)+1}: {doc_text}")
                current_tokens += doc_tokens
            else:
                remaining_tokens = max_context_tokens - current_tokens
                if remaining_tokens > 50:
                    truncated_tokens = self.tokenizer.encode(doc_text, add_special_tokens=False)[:remaining_tokens]
                    truncated_text = self.tokenizer.decode(truncated_tokens, skip_special_tokens=True)
                    context_parts.append(f"Document {len(context_parts)+1}: {truncated_text}...")
                break

        return "\n\n".join(context_parts)



    def generate_response(self, query: str, retrieved_docs: List[Dict]) -> Dict[str, Union[str, int]]:
        context = self.prepare_context(query, retrieved_docs)
        prompt = (
            "Based on the following context, answer the question accurately and concisely.\n\n"
            f"Context:\n{context}\n\n"
            f"Question: {query}\n\n"
            "Answer:"
        )

        try:
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=self.config.max_context_length,
                truncation=True,
                padding=True
            )
            attention_mask = inputs.get("attention_mask", None)

            with torch.no_grad():
                outputs = self.model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=attention_mask,
                    max_new_tokens=self.config.max_generation_length,
                    do_sample=False,
                    temperature=0.0,
                    top_p=0.9,
                    no_repeat_ngram_size=3,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )

            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            if "Answer:" in generated_text:
                answer = generated_text.split("Answer:")[-1].strip()
            else:
                answer = generated_text[len(prompt):].strip()

            return {
                'answer': answer,
                'prompt_tokens': int(inputs['input_ids'].shape[1]),
                'generated_tokens': int(outputs[0].shape[0] - inputs['input_ids'].shape[1]),
                'context_docs': len(retrieved_docs)
            }

        except Exception as e:
            print(f"Generation failed: {e}")
            return {
                'answer': f"I encountered an error generating a response: {str(e)}",
                'prompt_tokens': 0,
                'generated_tokens': 0,
                'context_docs': len(retrieved_docs)
            }

"""   # Gaurd Rail System  
  Implements both input and output guardrails
"""

class GuardrailSystem:
    """Implements both input and output guardrails"""

    def __init__(self, config: RAGConfig):
        self.config = config

        self.harmful_patterns = [
            r'how to (hack|break|crack|exploit)',
            r'(illegal|unlawful|criminal) (activity|acts|behavior)',
            r'(personal|private|confidential) (information|data)',
            r'(violence|harm|hurt|kill|murder)',
            r'(drugs|weapons|explosives)',
        ]

        self.low_quality_patterns = [
            r'^(.)\1{10,}',  # Repeated characters
            r'^\d+$',        # Only numbers
            r'^[^\w]*$',     # Only punctuation
        ]

        print("Guardrail system initialized")

    def validate_input(self, query: str) -> Dict[str, Union[bool, str, List[str]]]:
        issues = []

        if len(query) > self.config.max_query_length:
            issues.append(f"Query too long ({len(query)} > {self.config.max_query_length} chars)")
        if len(query.strip()) < 3:
            issues.append("Query too short")

        for pattern in self.harmful_patterns:
            if re.search(pattern, query.lower()):
                issues.append("Potentially harmful content detected")
                break

        alpha_ratio = sum(c.isalpha() for c in query) / len(query) if query else 0
        if alpha_ratio < 0.3:
            issues.append("Query appears to be gibberish")

        return {
            'is_valid': len(issues) == 0,
            'query': query,
            'issues': issues
        }

    def validate_output(self, answer: str, query: str) -> Dict[str, Union[bool, str, List[str], float]]:
        issues = []

        if len(answer.strip()) < self.config.min_answer_length:
            issues.append("Answer too short")

        for pattern in self.low_quality_patterns:
            if re.search(pattern, answer):
                issues.append("Repetitive or low-quality content detected")
                break

        # --- NEW: Financial number validation ---
        numbers = re.findall(r'\$?\s?\d[\d,]*(?:\.\d+)?\s?(?:million|billion|trillion)?', answer, re.IGNORECASE)

        if "revenue" in query.lower():
           if len(numbers) == 0:
             issues.append("No numerical value found in revenue-related answer")

           elif len(numbers) > 3:
             issues.append("Answer contains too many financial numbers, may be confusing")



        suspicious_patterns = [
            r'definitely|certainly|absolutely.*\$[\d,]+',
            r'(will|going to).*(\d{4}|\d{1,2}/\d{1,2}/\d{4})',
        ]
        for pattern in suspicious_patterns:
            if re.search(pattern, answer, re.IGNORECASE):
                issues.append("Potentially non-factual or overly certain statement")
                break

        query_words = set(re.findall(r'\b\w+\b', query.lower()))
        answer_words = set(re.findall(r'\b\w+\b', answer.lower()))
        overlap_ratio = len(query_words & answer_words) / len(query_words) if query_words else 0
        if overlap_ratio < 0.1:
            issues.append("Answer may not be relevant to the query")

        confidence = max(0.0, 1.0 - (len(issues) * 0.2))
        return {
            'is_valid': len(issues) == 0,
            'answer': answer,
            'issues': issues,
            'confidence': confidence
        }

"""# MAIN RAG PIPELINE
   Main RAG pipeline orchestrating all components

"""

class CompleteRAGPipeline:
    """Main RAG pipeline orchestrating all components"""

    def __init__(self, config: Optional[RAGConfig] = None):
        self.config = config or RAGConfig()

        print("Initializing Complete RAG Pipeline")
        print("=" * 50)

        self.preprocessor = QueryPreprocessor()
        self.retriever = MultiStageRetriever(self.config)
        self.generator = ResponseGenerator(self.config)
        self.guardrails = GuardrailSystem(self.config)

        self.is_initialized = False
        self.stats = {
            'total_queries': 0,
            'successful_queries': 0,
            'guardrail_violations': 0,
            'avg_response_time': 0.0
        }

        print("RAG Pipeline initialized successfully")

    def load_documents(self, documents: List[str], metadata: Optional[List[Dict]] = None):
        """Load, clean, chunk, and index documents"""
        print(f"Loading {len(documents)} raw documents...")
        flat_docs = []
        enriched_metadata = []

        for i, doc in enumerate(documents):
           if 'text' in doc:
              flat_docs.append(doc['text'])
              enriched_metadata.append(doc.get('metadata', {'id': i, 'type': 'paragraph', 'preview': doc['text'][:100]}))
           else:
              # Table row dict -> readable string
              doc_text = ' | '.join(f"{k}: {v}" for k, v in doc.items() if v)
              flat_docs.append(doc_text)
              enriched_metadata.append({'id': i, 'type': 'table_row', 'source': doc})

        #docs = clean_documents(documents)

        docs = clean_documents(flat_docs)

        # Chunking
        chunked_docs = []
        for d in docs:
            chunks = _chunk_text(d, self.config.chunk_size, self.config.chunk_overlap)
            chunked_docs.extend(chunks)

        print(f"Indexing {len(chunked_docs)} chunks...")
        self.retriever.build_indices(chunked_docs, metadata=None)
        self.is_initialized = True
        print("Documents loaded and indexed")
        
    def run_indexing(self, paths: List[Path], chunk_size: int, chunk_overlap: int):
        """
        Loads documents from file paths, processes them, builds the search indices,
        and returns statistics about the operation.
        """
        print(f"Starting indexing process for {len(paths)} files...")

        # 1. Load documents from the file paths using the global function
        # This function should be available in your script
        raw_docs = load_files_to_strings(paths)
        if not raw_docs:
            raise ValueError("No readable documents were found at the specified paths.")

        # 2. Extract text and perform chunking
        all_chunks = []
        for doc in raw_docs:
            # The load_files_to_strings function returns a list of dictionaries
            if 'text' in doc and isinstance(doc['text'], str):
                # Use the dynamic chunk_size and chunk_overlap passed from the UI
                chunks = _chunk_text(doc['text'], chunk_size, chunk_overlap)
                all_chunks.extend(chunks)

        print(f"Created {len(all_chunks)} chunks for indexing.")

        # 3. Build the indices using the retriever component
        # This calls the build_indices method you updated earlier to return the count
        chunk_count = self.retriever.build_indices(all_chunks, metadata=None)
        self.is_initialized = True
        print("Documents have been successfully indexed.")

        # 4. Return the final statistics in a dictionary for the Streamlit UI
        return {
            "message": f"Successfully indexed {len(paths)} files into {chunk_count} chunks.",
            "files_indexed": len(paths),
            "chunks_created": chunk_count
        }
    def query(self, user_query: str) -> Dict:
        """Process a complete query through the RAG pipeline"""

        if not self.is_initialized:
            return {
                'success': False,
                'error': "Pipeline not initialized. Load documents first.",
                'answer': '',
                'confidence': 0.0,
                'response_time': 0.0
            }

        start_time = time.time()
        try:
            self.stats['total_queries'] += 1

            if self.config.enable_input_guardrails:
                input_validation = self.guardrails.validate_input(user_query)
                if not input_validation['is_valid']:
                    self.stats['guardrail_violations'] += 1
                    return {
                        'success': False,
                        'error': f"Input validation failed: {', '.join(input_validation['issues'])}",
                        'answer': '',
                        'confidence': 0.0,
                        'response_time': time.time() - start_time
                    }

            preprocessed = self.preprocessor.preprocess(user_query)

            retrieved_docs = self.retriever.multi_stage_retrieve(preprocessed['cleaned'])
            direct_answer = extract_numeric_answer(user_query, retrieved_docs)
            print("[DEBUG] Direct answer ",direct_answer)
            if direct_answer:
                confidence = 0.95
                response_time = time.time() - start_time
                self.stats['successful_queries'] += 1
                self.stats['avg_response_time'] = (
                      self.stats['avg_response_time'] * (self.stats['total_queries'] - 1) + response_time
                        ) / self.stats['total_queries']

                return {
                   'success': True,
                   'answer': direct_answer,
                   'confidence': confidence,
                   'response_time': response_time,
                   'method': 'Direct Extraction',
                   'retrieved_docs': len(retrieved_docs),
                   'context_tokens': 0,
                   'generated_tokens': 0,
                   'preprocessing': preprocessed,
                   'retrieval_details': retrieved_docs,
                    'stats': self.get_statistics()
                  }

            generation_result = self.generator.generate_response(preprocessed['cleaned'], retrieved_docs)
            print("[DEBUG] Generation result ",generation_result)
            if self.config.enable_output_guardrails:
                output_validation = self.guardrails.validate_output(
                    generation_result['answer'],
                    user_query
                )
                if not output_validation['is_valid']:
                    self.stats['guardrail_violations'] += 1
                    modified_answer = f"{generation_result['answer']}\n\n[Guardrail flags: {', '.join(output_validation['issues'])}]"
                    confidence = output_validation['confidence']
                else:
                    modified_answer = generation_result['answer']
                    confidence = output_validation['confidence']
            else:
                modified_answer = generation_result['answer']
                confidence = 0.8

            response_time = time.time() - start_time
            self.stats['avg_response_time'] = (
                self.stats['avg_response_time'] * (self.stats['total_queries'] - 1) + response_time
            ) / self.stats['total_queries']
            self.stats['successful_queries'] += 1

            result = {
                'success': True,
                'answer': modified_answer,
                'confidence': confidence,
                'response_time': response_time,
                'method': 'Multi-Stage Retrieval (Hybrid + Cross-Encoder)',
                'retrieved_docs': len(retrieved_docs),
                'context_tokens': generation_result.get('prompt_tokens', 0),
                'generated_tokens': generation_result.get('generated_tokens', 0),
                'preprocessing': preprocessed,
                'retrieval_details': retrieved_docs,
                'stats': self.get_statistics()
            }
            return result

        except Exception as e:
            print(f"Pipeline error: {e}")
            return {
                'success': False,
                'error': f"Pipeline error: {str(e)}",
                'answer': '',
                'confidence': 0.0,
                'response_time': time.time() - start_time
            }

    def get_statistics(self) -> Dict:
        return {
            **self.stats,
            'success_rate': self.stats['successful_queries'] / max(self.stats['total_queries'], 1),
            'guardrail_violation_rate': self.stats['guardrail_violations'] / max(self.stats['total_queries'], 1)
        }

#if __name__ == "__main__":
    #cfg = RAGConfig()
    #pipeline = CompleteRAGPipeline(cfg)

    #ui = RAGInterface(pipeline)
    #app = ui.create_gradio_interface()
    #app.launch(share=True)   # share=False if running locally
    #app.launch(share=True, inline=False, inbrowser=True, debug=True)
